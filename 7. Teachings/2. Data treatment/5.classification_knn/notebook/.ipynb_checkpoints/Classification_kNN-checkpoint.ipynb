{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **Introduction to Classification. The k nearest neighbors method**\n",
    "\n",
    "    Notebook version: 1.0 (Oct 11, 2015)\n",
    "\n",
    "    Author: Jes√∫s Cid Sueiro (jcid@tsc.uc3m.es)\n",
    "\n",
    "    Changes: v.1.0 - First version\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import some libraries that will be necessary for working with data and displaying plots\n",
    "\n",
    "# To visualize plots in the notebook\n",
    "%matplotlib inline \n",
    "\n",
    "import csv     # To read csv files\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import spatial\n",
    "\n",
    "#import numpy as np\n",
    "#import scipy.io       # To read matlab files\n",
    "#import pylab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# 1. Introduction to Clasification.\n",
    "\n",
    "## 1.1 Introduction to the Classification problem\n",
    "\n",
    "In a generic classification problem, we are given an observation vector ${\\bf x}\\in \\mathbb{R}^N$ which is known to belong to one and only one *category* or *class*, $y$, in the set ${\\mathcal Y} = \\{0, 1, \\ldots, M-1\\}$. The goal of a classifier system is to predict the value of $y$ based on $x$.\n",
    "\n",
    "To design the classifier, we are given a collection of labelled observations ${\\mathcal S} = \\{({\\bf x}^{(k)}, y^{(k)}), k=1,\\ldots,K\\}$ where, for each observation ${\\bf x}^{(k)}$, the value of its true category, $y^{(k)}$, is known.\n",
    "\n",
    "We will focus in binary classification problems, where the label set is binary, ${\\mathcal Y} = \\{0, 1\\}$. Despite its simplicity, this is the most frequent case. Many multi-class classification problems are usually solved by decomposing them into a collection of binary problems.\n",
    "\n",
    "The classification algorithms, as many other machine learning algorithms, are based on two major underlying hypothesis:\n",
    "\n",
    "   - All samples in dataset ${\\mathcal S}$ are i.i.d. (independent and identically distributed), i.e., all samples are independent outcomes of an unknown distribution $p({\\bf x}, y)$.\n",
    "   - The tuple formed by the input sample and its unknown class, $({\\bf x}, y)$, is an independent outcome of the *same* distribution.\n",
    "   \n",
    "These two assumptions are essential to have some guarantees that a classifier designed based on ${\\mathcal S}$ has a good perfomance when applied to new input samples. Note that, despite we assume that an underlying distribution exists, it is unknwon: otherwise, we could apply classical decision theory to find the optimal predictor based on  $p({\\bf x}, y)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. The Iris dataset\n",
    "\n",
    "(The following presentation is based on this <a href=http://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/> Tutorial </a> by <a href=http://machinelearningmastery.com/about/> Jason Brownlee </a>) \n",
    "\n",
    "As an illustration, consider the <a href = http://archive.ics.uci.edu/ml/datasets/Iris> Iris dataset </a>, taken from the <a href=http://archive.ics.uci.edu/ml/> UCI Machine Learning repository </a>. Quoted from the dataset description:\n",
    "\n",
    "> This is perhaps the best known database to be found in the pattern recognition literature. The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. [...] One class is linearly separable from the other 2; the latter are NOT linearly separable from each other. \n",
    "\n",
    "The *class* is the species, which is one of *setosa*, *versicolor* or *virginica*. Each instance contains 4 measurements of given flowers: sepal length, sepal width, petal length and petal width, all in centimeters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.1, 3.5, 1.4, 0.2, Iris-setosa\n",
      "4.9, 3.0, 1.4, 0.2, Iris-setosa\n",
      "4.7, 3.2, 1.3, 0.2, Iris-setosa\n",
      "4.6, 3.1, 1.5, 0.2, Iris-setosa\n",
      "5.0, 3.6, 1.4, 0.2, Iris-setosa\n",
      "5.4, 3.9, 1.7, 0.4, Iris-setosa\n",
      "4.6, 3.4, 1.4, 0.3, Iris-setosa\n",
      "5.0, 3.4, 1.5, 0.2, Iris-setosa\n",
      "4.4, 2.9, 1.4, 0.2, Iris-setosa\n",
      "4.9, 3.1, 1.5, 0.1, Iris-setosa\n",
      "5.4, 3.7, 1.5, 0.2, Iris-setosa\n",
      "4.8, 3.4, 1.6, 0.2, Iris-setosa\n",
      "4.8, 3.0, 1.4, 0.1, Iris-setosa\n",
      "4.3, 3.0, 1.1, 0.1, Iris-setosa\n",
      "5.8, 4.0, 1.2, 0.2, Iris-setosa\n",
      "5.7, 4.4, 1.5, 0.4, Iris-setosa\n",
      "5.4, 3.9, 1.3, 0.4, Iris-setosa\n",
      "5.1, 3.5, 1.4, 0.3, Iris-setosa\n",
      "5.7, 3.8, 1.7, 0.3, Iris-setosa\n",
      "5.1, 3.8, 1.5, 0.3, Iris-setosa\n",
      "5.4, 3.4, 1.7, 0.2, Iris-setosa\n",
      "5.1, 3.7, 1.5, 0.4, Iris-setosa\n",
      "4.6, 3.6, 1.0, 0.2, Iris-setosa\n",
      "5.1, 3.3, 1.7, 0.5, Iris-setosa\n",
      "4.8, 3.4, 1.9, 0.2, Iris-setosa\n",
      "5.0, 3.0, 1.6, 0.2, Iris-setosa\n",
      "5.0, 3.4, 1.6, 0.4, Iris-setosa\n",
      "5.2, 3.5, 1.5, 0.2, Iris-setosa\n",
      "5.2, 3.4, 1.4, 0.2, Iris-setosa\n",
      "4.7, 3.2, 1.6, 0.2, Iris-setosa\n",
      "4.8, 3.1, 1.6, 0.2, Iris-setosa\n",
      "5.4, 3.4, 1.5, 0.4, Iris-setosa\n",
      "5.2, 4.1, 1.5, 0.1, Iris-setosa\n",
      "5.5, 4.2, 1.4, 0.2, Iris-setosa\n",
      "4.9, 3.1, 1.5, 0.1, Iris-setosa\n",
      "5.0, 3.2, 1.2, 0.2, Iris-setosa\n",
      "5.5, 3.5, 1.3, 0.2, Iris-setosa\n",
      "4.9, 3.1, 1.5, 0.1, Iris-setosa\n",
      "4.4, 3.0, 1.3, 0.2, Iris-setosa\n",
      "5.1, 3.4, 1.5, 0.2, Iris-setosa\n",
      "5.0, 3.5, 1.3, 0.3, Iris-setosa\n",
      "4.5, 2.3, 1.3, 0.3, Iris-setosa\n",
      "4.4, 3.2, 1.3, 0.2, Iris-setosa\n",
      "5.0, 3.5, 1.6, 0.6, Iris-setosa\n",
      "5.1, 3.8, 1.9, 0.4, Iris-setosa\n",
      "4.8, 3.0, 1.4, 0.3, Iris-setosa\n",
      "5.1, 3.8, 1.6, 0.2, Iris-setosa\n",
      "4.6, 3.2, 1.4, 0.2, Iris-setosa\n",
      "5.3, 3.7, 1.5, 0.2, Iris-setosa\n",
      "5.0, 3.3, 1.4, 0.2, Iris-setosa\n",
      "7.0, 3.2, 4.7, 1.4, Iris-versicolor\n",
      "6.4, 3.2, 4.5, 1.5, Iris-versicolor\n",
      "6.9, 3.1, 4.9, 1.5, Iris-versicolor\n",
      "5.5, 2.3, 4.0, 1.3, Iris-versicolor\n",
      "6.5, 2.8, 4.6, 1.5, Iris-versicolor\n",
      "5.7, 2.8, 4.5, 1.3, Iris-versicolor\n",
      "6.3, 3.3, 4.7, 1.6, Iris-versicolor\n",
      "4.9, 2.4, 3.3, 1.0, Iris-versicolor\n",
      "6.6, 2.9, 4.6, 1.3, Iris-versicolor\n",
      "5.2, 2.7, 3.9, 1.4, Iris-versicolor\n",
      "5.0, 2.0, 3.5, 1.0, Iris-versicolor\n",
      "5.9, 3.0, 4.2, 1.5, Iris-versicolor\n",
      "6.0, 2.2, 4.0, 1.0, Iris-versicolor\n",
      "6.1, 2.9, 4.7, 1.4, Iris-versicolor\n",
      "5.6, 2.9, 3.6, 1.3, Iris-versicolor\n",
      "6.7, 3.1, 4.4, 1.4, Iris-versicolor\n",
      "5.6, 3.0, 4.5, 1.5, Iris-versicolor\n",
      "5.8, 2.7, 4.1, 1.0, Iris-versicolor\n",
      "6.2, 2.2, 4.5, 1.5, Iris-versicolor\n",
      "5.6, 2.5, 3.9, 1.1, Iris-versicolor\n",
      "5.9, 3.2, 4.8, 1.8, Iris-versicolor\n",
      "6.1, 2.8, 4.0, 1.3, Iris-versicolor\n",
      "6.3, 2.5, 4.9, 1.5, Iris-versicolor\n",
      "6.1, 2.8, 4.7, 1.2, Iris-versicolor\n",
      "6.4, 2.9, 4.3, 1.3, Iris-versicolor\n",
      "6.6, 3.0, 4.4, 1.4, Iris-versicolor\n",
      "6.8, 2.8, 4.8, 1.4, Iris-versicolor\n",
      "6.7, 3.0, 5.0, 1.7, Iris-versicolor\n",
      "6.0, 2.9, 4.5, 1.5, Iris-versicolor\n",
      "5.7, 2.6, 3.5, 1.0, Iris-versicolor\n",
      "5.5, 2.4, 3.8, 1.1, Iris-versicolor\n",
      "5.5, 2.4, 3.7, 1.0, Iris-versicolor\n",
      "5.8, 2.7, 3.9, 1.2, Iris-versicolor\n",
      "6.0, 2.7, 5.1, 1.6, Iris-versicolor\n",
      "5.4, 3.0, 4.5, 1.5, Iris-versicolor\n",
      "6.0, 3.4, 4.5, 1.6, Iris-versicolor\n",
      "6.7, 3.1, 4.7, 1.5, Iris-versicolor\n",
      "6.3, 2.3, 4.4, 1.3, Iris-versicolor\n",
      "5.6, 3.0, 4.1, 1.3, Iris-versicolor\n",
      "5.5, 2.5, 4.0, 1.3, Iris-versicolor\n",
      "5.5, 2.6, 4.4, 1.2, Iris-versicolor\n",
      "6.1, 3.0, 4.6, 1.4, Iris-versicolor\n",
      "5.8, 2.6, 4.0, 1.2, Iris-versicolor\n",
      "5.0, 2.3, 3.3, 1.0, Iris-versicolor\n",
      "5.6, 2.7, 4.2, 1.3, Iris-versicolor\n",
      "5.7, 3.0, 4.2, 1.2, Iris-versicolor\n",
      "5.7, 2.9, 4.2, 1.3, Iris-versicolor\n",
      "6.2, 2.9, 4.3, 1.3, Iris-versicolor\n",
      "5.1, 2.5, 3.0, 1.1, Iris-versicolor\n",
      "5.7, 2.8, 4.1, 1.3, Iris-versicolor\n",
      "6.3, 3.3, 6.0, 2.5, Iris-virginica\n",
      "5.8, 2.7, 5.1, 1.9, Iris-virginica\n",
      "7.1, 3.0, 5.9, 2.1, Iris-virginica\n",
      "6.3, 2.9, 5.6, 1.8, Iris-virginica\n",
      "6.5, 3.0, 5.8, 2.2, Iris-virginica\n",
      "7.6, 3.0, 6.6, 2.1, Iris-virginica\n",
      "4.9, 2.5, 4.5, 1.7, Iris-virginica\n",
      "7.3, 2.9, 6.3, 1.8, Iris-virginica\n",
      "6.7, 2.5, 5.8, 1.8, Iris-virginica\n",
      "7.2, 3.6, 6.1, 2.5, Iris-virginica\n",
      "6.5, 3.2, 5.1, 2.0, Iris-virginica\n",
      "6.4, 2.7, 5.3, 1.9, Iris-virginica\n",
      "6.8, 3.0, 5.5, 2.1, Iris-virginica\n",
      "5.7, 2.5, 5.0, 2.0, Iris-virginica\n",
      "5.8, 2.8, 5.1, 2.4, Iris-virginica\n",
      "6.4, 3.2, 5.3, 2.3, Iris-virginica\n",
      "6.5, 3.0, 5.5, 1.8, Iris-virginica\n",
      "7.7, 3.8, 6.7, 2.2, Iris-virginica\n",
      "7.7, 2.6, 6.9, 2.3, Iris-virginica\n",
      "6.0, 2.2, 5.0, 1.5, Iris-virginica\n",
      "6.9, 3.2, 5.7, 2.3, Iris-virginica\n",
      "5.6, 2.8, 4.9, 2.0, Iris-virginica\n",
      "7.7, 2.8, 6.7, 2.0, Iris-virginica\n",
      "6.3, 2.7, 4.9, 1.8, Iris-virginica\n",
      "6.7, 3.3, 5.7, 2.1, Iris-virginica\n",
      "7.2, 3.2, 6.0, 1.8, Iris-virginica\n",
      "6.2, 2.8, 4.8, 1.8, Iris-virginica\n",
      "6.1, 3.0, 4.9, 1.8, Iris-virginica\n",
      "6.4, 2.8, 5.6, 2.1, Iris-virginica\n",
      "7.2, 3.0, 5.8, 1.6, Iris-virginica\n",
      "7.4, 2.8, 6.1, 1.9, Iris-virginica\n",
      "7.9, 3.8, 6.4, 2.0, Iris-virginica\n",
      "6.4, 2.8, 5.6, 2.2, Iris-virginica\n",
      "6.3, 2.8, 5.1, 1.5, Iris-virginica\n",
      "6.1, 2.6, 5.6, 1.4, Iris-virginica\n",
      "7.7, 3.0, 6.1, 2.3, Iris-virginica\n",
      "6.3, 3.4, 5.6, 2.4, Iris-virginica\n",
      "6.4, 3.1, 5.5, 1.8, Iris-virginica\n",
      "6.0, 3.0, 4.8, 1.8, Iris-virginica\n",
      "6.9, 3.1, 5.4, 2.1, Iris-virginica\n",
      "6.7, 3.1, 5.6, 2.4, Iris-virginica\n",
      "6.9, 3.1, 5.1, 2.3, Iris-virginica\n",
      "5.8, 2.7, 5.1, 1.9, Iris-virginica\n",
      "6.8, 3.2, 5.9, 2.3, Iris-virginica\n",
      "6.7, 3.3, 5.7, 2.5, Iris-virginica\n",
      "6.7, 3.0, 5.2, 2.3, Iris-virginica\n",
      "6.3, 2.5, 5.0, 1.9, Iris-virginica\n",
      "6.5, 3.0, 5.2, 2.0, Iris-virginica\n",
      "6.2, 3.4, 5.4, 2.3, Iris-virginica\n",
      "5.9, 3.0, 5.1, 1.8, Iris-virginica\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Taken from Jason Brownlee notebook.\n",
    "with open('iris.data', 'rb') as csvfile:\n",
    "\tlines = csv.reader(csvfile)\n",
    "\tfor row in lines:\n",
    "\t\tprint ', '.join(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to split the data into a training dataset that kNN can use to make predictions and a test dataset that we can use to evaluate the accuracy of the model.\n",
    "\n",
    "We first need to convert the flower measures that were loaded as strings into numbers that we can work with. Next we need to split the data set randomly into train and datasets. A ratio of 67/33 for train/test is a standard ratio used.\n",
    "\n",
    "Pulling it all together, we can define a function called loadDataset that loads a CSV with the provided filename and splits it randomly into train and test datasets using the provided split ratio.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from a notebook by Jason Brownlee\n",
    "def loadDataset(filename, split):\n",
    "    xTrain = []\n",
    "    cTrain = []\n",
    "    xTest = []\n",
    "    cTest = []\n",
    "\n",
    "    with open(filename, 'rb') as csvfile:\n",
    "        lines = csv.reader(csvfile)\n",
    "        dataset = list(lines)\n",
    "    for i in range(len(dataset)-1):\n",
    "        for y in range(4):\n",
    "            dataset[i][y] = float(dataset[i][y])\n",
    "        item = dataset[i]\n",
    "        if random.random() < split:\n",
    "            xTrain.append(item[0:4])\n",
    "            cTrain.append(item[4])\n",
    "        else:\n",
    "            xTest.append(item[0:4])\n",
    "            cTest.append(item[4])\n",
    "    return xTrain, cTrain, xTest, cTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this function to get a data split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 95\n",
      "Test: 55\n"
     ]
    }
   ],
   "source": [
    "xTrain_all, cTrain_all, xTest_all, cTest_all = loadDataset('iris.data', 0.66)\n",
    "nTrain_all = len(xTrain_all)\n",
    "nTest_all = len(xTest_all)\n",
    "print 'Train: ' + str(nTrain_all)\n",
    "print 'Test: ' + str(nTest_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get some intuition about this four dimensional dataset we can plot 2-dimensional projections taking only two variables each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1054eb6d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAERCAYAAACQIWsgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/j8m712AaD+e5QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10537cd90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 0 # Try 0,1,2,3\n",
    "j = 1 # Try 0,1,2,3 with j!=i\n",
    "\n",
    "# Take coordinates for each class separately\n",
    "xiSe = [xTrain_all[n][i] for n in range(nTrain_all) if cTrain_all[n]=='Iris-setosa']\n",
    "xjSe = [xTrain_all[n][j] for n in range(nTrain_all) if cTrain_all[n]=='Iris-setosa']\n",
    "xiVe = [xTrain_all[n][i] for n in range(nTrain_all) if cTrain_all[n]=='Iris-versicolor']\n",
    "xjVe = [xTrain_all[n][j] for n in range(nTrain_all) if cTrain_all[n]=='Iris-versicolor']\n",
    "xiVi = [xTrain_all[n][i] for n in range(nTrain_all) if cTrain_all[n]=='Iris-virginica']\n",
    "xjVi = [xTrain_all[n][j] for n in range(nTrain_all) if cTrain_all[n]=='Iris-virginica']\n",
    "\n",
    "plt.plot(xiSe, xjSe,'r.', label='Setosa')\n",
    "plt.plot(xiVe, xjVe,'g+', label='Versicolor')\n",
    "plt.plot(xiVi, xjVi,'bx', label='Virginica')\n",
    "plt.xlabel('$x_' + str(i) + '$')\n",
    "plt.ylabel('$x_' + str(j) + '$')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will design a classifier to separate classes \"Versicolor\" and \"Virginica\" using $x_0$ and $x_1$ only. To do so, we build a training set with samples from these categories, and a bynary label $y^{(k)} = 1$ for samples in class \"Virginica\", and $0$ for \"Versicolor\" data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x105b55790>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAERCAYAAACQIWsgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUXEQEZEQFQcREQn5/6smlkyK7xKoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x105585c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select two classes\n",
    "c0 = 'Iris-versicolor' \n",
    "c1 = 'Iris-virginica'\n",
    "\n",
    "# Select two coordinates\n",
    "ind = [0, 1]\n",
    "\n",
    "# Take training test\n",
    "X_tr = [[xTrain_all[n][i] for i in ind] for n in range(nTrain_all) \n",
    "          if cTrain_all[n]==c0 or cTrain_all[n]==c1]\n",
    "C_tr = [cTrain_all[n] for n in range(nTrain_all) \n",
    "          if cTrain_all[n]==c0 or cTrain_all[n]==c1]\n",
    "Y_tr = [int(c==c1) for c in C_tr]\n",
    "n_tr = len(X_tr)\n",
    "\n",
    "# Take test set\n",
    "X_tst = [[xTest_all[n][i] for i in ind] for n in range(nTest_all) \n",
    "          if cTest_all[n]==c0 or cTest_all[n]==c1]\n",
    "C_tst = [cTest_all[n] for n in range(nTest_all) \n",
    "         if cTest_all[n]==c0 or cTest_all[n]==c1]\n",
    "Y_tst = [int(c==c1) for c in C_tst]\n",
    "n_tst = len(X_tst)\n",
    "\n",
    "# In this section we will plot together the square and absolute errors\n",
    "x0c0 = [X_tr[n][0] for n in range(n_tr) if Y_tr[n]==0]\n",
    "x1c0 = [X_tr[n][1] for n in range(n_tr) if Y_tr[n]==0]\n",
    "x0c1 = [X_tr[n][0] for n in range(n_tr) if Y_tr[n]==1]\n",
    "x1c1 = [X_tr[n][1] for n in range(n_tr) if Y_tr[n]==1]\n",
    "\n",
    "labels = {'Iris-setosa': 'Setosa', \n",
    "          'Iris-versicolor': 'Versicolor',\n",
    "          'Iris-virginica': 'Virginica'}\n",
    "plt.plot(x0c0, x1c0,'r.', label=labels[c0])\n",
    "plt.plot(x0c1, x1c1,'g+', label=labels[c1])\n",
    "plt.xlabel('$x_' + str(ind[0]) + '$')\n",
    "plt.ylabel('$x_' + str(ind[1]) + '$')\n",
    "plt.legend(loc='best')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Baseline Classifier: Maximum A Priori.\n",
    "\n",
    "For the selected data set, we have two clases and a dataset with the following class proportions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 (Iris-versicolor): 28 samples\n",
      "Class 1 (Iris-virginica): 34 samples\n"
     ]
    }
   ],
   "source": [
    "print 'Class 0 (' + c0 + '): ' + str(sum(Y_tr)) + ' samples'\n",
    "print 'Class 1 (' + c1 + '): ' + str(n_tr - sum(Y_tr)) + ' samples'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum a priori classifier assigns any sample ${\\bf x}$ to the most frequent class in the training set. Therefore, the ouput $y$ for any sample ${\\bf x}$ is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 0 (Iris-versicolor)\n"
     ]
    }
   ],
   "source": [
    "y = int(2*sum(Y_tr) > n_tr)\n",
    "print 'y = ' + str(y) + ' (' + (c1 if y==1 else c0) + ')'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error rate for these baseline classifier is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pe(train) = 0.451612903226\n",
      "Pe(test) = 0.578947368421\n"
     ]
    }
   ],
   "source": [
    "# Training and test error arrays\n",
    "E_tr = [z!=y for z in Y_tr]\n",
    "E_tst = [z!=y for z in Y_tst]\n",
    "\n",
    "# Error rates\n",
    "pe_tr = float(sum(E_tr)) / n_tr\n",
    "pe_tst = float(sum(E_tst)) / n_tst\n",
    "print 'Pe(train) = ' + str(pe_tr)\n",
    "print 'Pe(test) = ' + str(pe_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error rate of the baseline classifier is a simple benchmark for classification. Since the maximum a priori decision is independent on the observation, ${\\bf x}$, any classifier based on ${\\bf x}$ should have a better (or, at least, not worse) performance than the baseline classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The Nearest-Neighbour Classifier (1-NN).\n",
    "\n",
    "The 1-NN classifier assigns any instance ${\\bf x}$ to the category of the nearest neighbour in the training set.\n",
    "$$\n",
    "d = f({\\bf x}) = y^{(n)} \\\\\n",
    "n = \\arg \\min_k \\|{\\bf x}-{\\bf x}^{(k)}\\|\n",
    "$$\n",
    "In case of ties (i.e. if there is more than one instance at minimum distance) the class of one of them, taken arbitrarily, is assigned to ${\\bf x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn_classifier(X1,S1,X2):\n",
    "    \"\"\" Compute the 1-NN classification for the observations contained in\n",
    "        the rows of X2, for the training set given by the rows in X1 and the\n",
    "        components of S1.\n",
    "    \"\"\"\n",
    "    if X1.ndim == 1:\n",
    "        X1 = np.asmatrix(X1).T\n",
    "    if X2.ndim == 1:\n",
    "        X2 = np.asmatrix(X2).T\n",
    "    distances = spatial.distance.cdist(X1,X2,'euclidean')\n",
    "    neighbors = np.argsort(distances, axis=0, kind='quicksort', order=None)\n",
    "    closest = neighbors[0,:]\n",
    "\n",
    "    y_values = np.zeros([X2.shape[0],1])\n",
    "    for idx in range(X2.shape[0]):\n",
    "        y_values[idx] = np.mean(S1[closest[:,idx]])\n",
    "        \n",
    "    return y_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-debe1f78a14a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#For representational purposes, we will compute the output of the regression model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#in a series of equally spaced-points along the x-axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mgrid_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mgrid_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mX_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_min\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrid_max\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# We implement unidimensional regression using the k-nn method\n",
    "# In other words, the estimations are to be made using only one variable at a time\n",
    "\n",
    "k = 1 #Number of neighbors\n",
    "n_points = 1000 #Number of points in the 'x' axis (for representational purposes)\n",
    "\n",
    "#For representational purposes, we will compute the output of the regression model\n",
    "#in a series of equally spaced-points along the x-axis\n",
    "grid_min = np.min([np.min(X_tr[:,var]), np.min(X_tst[:,var])])\n",
    "grid_max = np.max([np.max(X_tr[:,var]), np.max(X_tst[:,var])])\n",
    "X_grid = np.linspace(grid_min,grid_max,num=n_points)\n",
    "\n",
    "\n",
    "\n",
    "est_tst = knn_regression(X_tr[:,var],S_tr,X_tst,k)\n",
    "est_grid = knn_regression(X_tr[:,var],S_tr,X_grid,k)\n",
    "\n",
    "plt.plot(X_tr[:,var],S_tr,'b.',label='Training points')\n",
    "plt.plot(X_tst[:,var],S_tst,'rx',label='Test points')\n",
    "plt.plot(X_grid,est_grid,'g-',label='Regression model')\n",
    "plt.axis('tight')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def knn_regression(X1,S1,X2,k):\n",
    "    \"\"\" Compute the k-NN regression estimate for the observations contained in\n",
    "        the rows of X2, for the training set given by the rows in X1 and the\n",
    "        components of S1. k is the number of neighbours of the k-NN algorithm\n",
    "    \"\"\"\n",
    "    if X1.ndim == 1:\n",
    "        X1 = np.asmatrix(X1).T\n",
    "    if X2.ndim == 1:\n",
    "        X2 = np.asmatrix(X2).T\n",
    "    distances = spatial.distance.cdist(X1,X2,'euclidean')\n",
    "    neighbors = np.argsort(distances, axis=0, kind='quicksort', order=None)\n",
    "    closest = neighbors[range(k),:]\n",
    "    \n",
    "    est_values = np.zeros([X2.shape[0],1])\n",
    "    for idx in range(X2.shape[0]):\n",
    "        est_values[idx] = np.mean(S1[closest[:,idx]])\n",
    "        \n",
    "    return est_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Unidimensional regression with the k-nn method\n",
    "\n",
    "The principles of the k-nn method are as follows:\n",
    "\n",
    "   - For each point where a prediction is to be made, find the $k$ closest neighbors to that point (in the training set)\n",
    "   - Obtain the estimation averaging the labels corresponding to the selected neighbors\n",
    "   \n",
    "The number of neighbors is a hyperparameter that plays an important role in the performance of the method. You can test its influence by changing $k$ in the following piece of code. In particular, you can sart with $k=1$ and observe the efect of increasing the value of $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolution of the error with the number of neighbors ($k$)\n",
    "\n",
    "We see that a small $k$ results in a regression curve that exhibits many and large oscillations.  The curve is capturing any noise that may be present in the training data, and <i>overfits</i> the training set. On the other hand, picking a too large $k$ (e.g., 200) the regression curve becomes too smooth, averaging out the values of the labels in the training set over large intervals of the observation variable.\n",
    "\n",
    "The next code illustrates this effect by plotting the average training and test square errors as a function of $k$. As we can see, the error initially decreases achiving a minimum (in the test set) for $k\\approx 10$. Increasing the value of $k$ beyond that value results in poorer performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var = 0\n",
    "k_max = 40\n",
    "\n",
    "#Be careful with the use of range, e.g., range(3) = [0, 1, 2]\n",
    "sq_error_tr = [square_error(S_tr,knn_regression(X_tr[:,var],S_tr,X_tr[:,var],k+1)) for k in range(k_max)]\n",
    "sq_error_tst = [square_error(S_tst,knn_regression(X_tr[:,var],S_tr,X_tst[:,var],k+1)) for k in range(k_max)]\n",
    "\n",
    "plt.plot(np.arange(k_max)+1,sq_error_tr,'b-',label='Training square error')\n",
    "plt.plot(np.arange(k_max)+1,sq_error_tst,'r',label='Test square error')\n",
    "plt.axis('tight')\n",
    "\n",
    "plt.legend(loc='best')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion topics and exercises: \n",
    "\n",
    "1. Analize the training square error for $k=0$\n",
    "2. Modify the code above to visualize the square error from $k=1$ up to $k$ equal to the number of training instances. Can you relate the square error of the $k$-NN method with that of the baseline method for certain value of $k$? \n",
    "3. Modify the code above to print on the screen the exact value of $k$ minimizing the test square error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###¬†Analysis of the performance for the different variables\n",
    "\n",
    "Having a look at the scatter plots, we can observe that some observation variables seem to have a more clear relationship with the target value. For instance, variable 1 seems to have a linear correlation, in spite of the noise. Thus, we can expect that not all variables are equally useful for the regression task. In the following plot, we carry out a study of the performance that can be achieved with each variable. Since the test labels cannot be used to select the hyperparamenter $k$, we will pick the value that minimizes the training error (for each variable). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_max = 20\n",
    "\n",
    "var_performance = []\n",
    "k_values = []\n",
    "\n",
    "for var in range(X_tr.shape[1]):\n",
    "    sq_error_tr = [square_error(S_tr,knn_regression(X_tr[:,var],S_tr,X_tr[:,var],k+1)) for k in range(k_max)]\n",
    "    sq_error_tst = [square_error(S_tst,knn_regression(X_tr[:,var],S_tr,X_tst[:,var],k+1)) for k in range(k_max)]\n",
    "    sq_error_tr = np.asarray(sq_error_tr)\n",
    "    sq_error_tst = np.asarray(sq_error_tst)\n",
    "\n",
    "    # We select the variable associated to the value of k for which the training error is minimum\n",
    "    pos = np.argmin(sq_error_tr)\n",
    "    k_values.append(pos+1)\n",
    "    var_performance.append(sq_error_tst[pos])\n",
    "    \n",
    "plt.stem(range(X_tr.shape[1]),var_performance)\n",
    "plt.title('Results of unidimensional regression (kNN)')\n",
    "plt.xlabel('Variable')\n",
    "plt.ylabel('Quadratic error')\n",
    "\n",
    "plt.figure(2)\n",
    "plt.stem(range(X_tr.shape[1]),k_values)\n",
    "plt.xlabel('Variable')\n",
    "plt.ylabel('k')\n",
    "plt.title('Selection of the hyperparameter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##¬†1.5 Multidimensional regression with the k-nn method\n",
    "\n",
    "In the previous subsection, we have studied the performance of the k-nn method when using only one variable. Doing so was convenient, because it allowed us to plot the regression curves in a 2-D plot, and to get some insight about the consequences of modifying the number of neighbors.\n",
    "\n",
    "For completeness, we evaluate now the performance of the k-nn method in this dataset when using all variables together. In fact, when designing a regression model, we should proceed in this manner, using all available information to make as accurate an estimation as possible. In this way, we can also account for correlations that might be present among the different observation variables, and that may carry very relevant information for the regression task.\n",
    "\n",
    "For instance, in the `STOCK` dataset, it may be that the combination of the stock values of two airplane companies is more informative about the price of the target company, while the value for a single company is not enough.\n",
    "\n",
    "<small> Also, in the `CONCRETE` dataset, it may be that for the particular problem at hand the combination of a large proportion of water and a small proportion of coarse grain is a clear indication of certain compressive strength of the material, while the proportion of water or coarse grain alone are not enough to get to that result.<\\small>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_max = 20\n",
    "\n",
    "sq_error_tr = [square_error(S_tr,knn_regression(X_tr,S_tr,X_tr,k+1)) for k in range(k_max)]\n",
    "sq_error_tst = [square_error(S_tst,knn_regression(X_tr,S_tr,X_tst,k+1)) for k in range(k_max)]\n",
    "\n",
    "plt.plot(np.arange(k_max)+1,sq_error_tr,'b-',label='Training square error')\n",
    "plt.plot(np.arange(k_max)+1,sq_error_tst,'r--',label='Test square error')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Square error')\n",
    "\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can check that the average test square error is much lower than the error that was achieved when using only one variable, and also far better than the baseline method. It is also interesting to note that in this particular case the best performance is achieved for a small value of $k$, with the error increasing for larger values of the hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Hyperparameter selection via cross-validation\n",
    "\n",
    "An inconvenient of the application of the $k$-nn method is that the selection of $k$ influences the final error of the algorithm. In the previous experiments, we kept the value of $k$ that minimized the square error on the training set. However, we also noticed that the location of the minimum is not necessarily the same from the perspective of the test data. Ideally, we would like that the designed regression model works as well as possible on future unlabeled patterns that are not available during the training phase. This property is known as <i>generalization</i>. Fitting the training data is only pursued in the hope that we are also indirectly obtaining a model that generalizes well. In order to achieve this goal, there are some strategies that try to guarantee a correct generalization of the model. One of such approaches is known as <b>cross-validation</b> \n",
    "\n",
    "Since using the test labels during the training phase is not allowed (they should be kept aside to simultate the future application of the regression model on unseen patterns), we need to figure out some way to improve our estimation of the hyperparameter that requires only training data. Cross-validation allows us to do so by following the following steps:\n",
    "\n",
    "   - Split the training data into several (generally non-overlapping) subsets. If we use M subsets, the method is referred to as M-fold cross-validation. If we consider each pattern a different subset, the method is usually referred to as leave-one-out (LOO) cross-validation.\n",
    "   - Carry out the training of the system $M$ times. For each run, use a different partition as a <i>validation</i> set, and use the restating partitions as the training set. Evaluate the performance for different choices of the hyperparameter (i.e., for different values of $k$ for the k-NN method).\n",
    "   - Average the validation error over all partitions, and pick the hyperparameter that provided the minimum validation error.\n",
    "   - Rerun the algorithm using all the training data, keeping the value of the parameter that came out of the cross-validation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## This fragment of code runs k-nn with M-fold cross validation\n",
    "\n",
    "#¬†Obtain the indices for the different folds\n",
    "n_tr = X_tr.shape[0]\n",
    "M = 3\n",
    "permutation = np.random.permutation(n_tr)\n",
    "\n",
    "#Not very smart, but it works\n",
    "set_indices = {}\n",
    "for k in range(M):\n",
    "    set_indices[k] = []\n",
    "k = 0\n",
    "for pos in range(n_tr):\n",
    "    set_indices[k].append(permutation[pos])\n",
    "    k = (k+1) % M\n",
    "    \n",
    "# Now, we run the cross-validation process using the k-nn method\n",
    "var = 0\n",
    "k_max = 40\n",
    "\n",
    "# Obtain the validation errors\n",
    "sq_error_val = np.zeros((1,k_max))\n",
    "for k in range(M):\n",
    "    val_indices = set_indices[k]\n",
    "    tr_indices = []\n",
    "    for kk in range(M):\n",
    "        if not k==kk:\n",
    "            tr_indices += set_indices[kk]\n",
    "    \n",
    "    sq_error_val_iter = [square_error(S_tr[val_indices],knn_regression(X_tr[tr_indices,var],S_tr[tr_indices] \\\n",
    "                                                                       ,X_tr[val_indices,var],k+1)) for k in range(k_max)]\n",
    "    sq_error_val = sq_error_val + np.asarray(sq_error_val_iter).T\n",
    "    \n",
    "sq_error_val = sq_error_val/M\n",
    "\n",
    "# We compute now the train and test errors curves\n",
    "sq_error_tr = [square_error(S_tr,knn_regression(X_tr[:,var],S_tr,X_tr[:,var],k+1)) for k in range(k_max)]\n",
    "sq_error_tst = [square_error(S_tst,knn_regression(X_tr[:,var],S_tr,X_tst[:,var],k+1)) for k in range(k_max)]\n",
    "\n",
    "plt.plot(np.arange(k_max)+1,sq_error_tr,'b-',label='Training square error')\n",
    "plt.plot(np.arange(k_max)+1,sq_error_tst,'r--',label='Test square error')\n",
    "plt.plot(np.arange(k_max)+1,sq_error_val.T,'g--',label='Validation square error')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Scikit-learn implementation\n",
    "\n",
    "In practice, most well-known machine learning methods are implemented and available for python. Probably, the most complete module for machine learning tools is <a href=http://scikit-learn.org/stable/>Scikit-learn</a>. The following piece of code uses the method\n",
    "\n",
    "    KNeighborsRegressor\n",
    "   \n",
    "available in Scikit-learn. The example has been taken from <a href=http://scikit-learn.org/stable/auto_examples/neighbors/plot_regression.html>here</a>. As you can check, this routine allows us to build the estimation for a particular point using a weighted average of the targets of the neighbors:\n",
    "\n",
    "   To obtain the estimation at a point ${\\bf x}$:\n",
    "   \n",
    "   - Find $k$ closest points to ${\\bf x}$ in the training set\n",
    "   - Average the corresponding targets, weighting each value according to the distance of each point to ${\\bf x}$, so that closer points have a larger influence in the estimation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n",
    "#         Fabian Pedregosa <fabian.pedregosa@inria.fr>\n",
    "#\n",
    "# License: BSD 3 clause (C) INRIA\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Generate sample data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import neighbors\n",
    "\n",
    "np.random.seed(0)\n",
    "X = np.sort(5 * np.random.rand(40, 1), axis=0)\n",
    "T = np.linspace(0, 5, 500)[:, np.newaxis]\n",
    "y = np.sin(X).ravel()\n",
    "\n",
    "# Add noise to targets\n",
    "y[::5] += 1 * (0.5 - np.random.rand(8))\n",
    "\n",
    "###############################################################################\n",
    "# Fit regression model\n",
    "n_neighbors = 5\n",
    "\n",
    "for i, weights in enumerate(['uniform', 'distance']):\n",
    "    knn = neighbors.KNeighborsRegressor(n_neighbors, weights=weights)\n",
    "    y_ = knn.fit(X, y).predict(T)\n",
    "\n",
    "    plt.subplot(2, 1, i + 1)\n",
    "    plt.scatter(X, y, c='k', label='data')\n",
    "    plt.plot(T, y_, c='g', label='prediction')\n",
    "    plt.axis('tight')\n",
    "    plt.legend()\n",
    "    plt.title(\"KNeighborsRegressor (k = %i, weights = '%s')\" % (n_neighbors,\n",
    "                                                                weights))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Use scikit-learn implementation of the $k$-nn method to compute the generalization error on the `CONCRETE` dataset. Compare the perfomance when using uniform and distance-based weights in the computation the estimates. Visualize the regression curves and error for different values of $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
